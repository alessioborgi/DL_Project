Trainable Information Retrieval (IR) systems are characterized by two phases:
\begin{itemize}
  \item \textbf{Indexing: }Indexing of a corpus, which means to associate the content of each document with its corresponding docid.
  \item \textbf{Retrieval: }Learn how to retrieve efficiently from the index, which means to 
\end{itemize}
Instead of using Contrastive Learning based Dual Encoders, the paper proposes an architecture which directly map a query \textbf{q} to a relevant docid \textbf{j}. This architecture, called DSI, it's implemented with a pre-trained Transformer and all the information of the corpus are encoded within the parameters of the language model. When we are doing inference, the give to the trained model a text query as input and we expect to obtain a docid as output. If we are interested in a ranked list of relevant documents we can also use Beam Search. Our DSI system uses standard model inference to map from encodings to docids, instead of learning internal representations that optimize a search procedure. DSI can be extended in different ways:
\begin{itemize}
  \item \textbf{Document representation: }there are several ways to represent documents (e.g. full text, bag-of-words representations, ...)
  \item \textbf{Docid representation: }(e.g. unique tokens, structured semantic docids, text strings, ...)
\end{itemize}

\subsection{Indexing Methods}
Given a sequence of document tokens, the model is trained to predict the docids. There can be used different strategies:
\begin{itemize}
  \item \textbf{Inputs2Target}: 
  \item \textbf{Targets2Inputs}:
  \item \textbf{Bidirectional}: 
\end{itemize}
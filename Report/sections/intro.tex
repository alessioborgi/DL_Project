Traditional information retrieval (IR) approaches, which often rely on static indexing and matching techniques, struggle to scale in dynamic, large and complex data environments. Neural Information Retrieval (NIR) methods, that leverages deep learning, have emerged allwing models to learn semantic representations and retrieve information with huge results. Among these, the Differentiable Search Index (DSI) stands out as a neural inverted index that maps document identifiers to semantic embeddings, offering an alternative to traditional IR paradigms.


The original DSI model faces challenges in adapting to diverse data distributions and resource constraints. To address these limitations, we enhance the DSI framework through both data augmentation and model optimization techniques. First, we introduce three novel data augmentation strategies tailored to enrich the input data: (1) Num2Word, which transforms numerical values into their word equivalents to standardize representation and improve semantic understanding; (2) stopwords removal, which eliminates non-informative words to focus on meaningful content; and (3) POS-MLM, a masked language modeling approach guided by part-of-speech tags to reinforce syntactic and semantic learning.

In parallel, we develop and evaluate four advanced variants of the DSI model to optimize its performance further. These include:
\begin{itemize}
    \item DSI+LoRA: Incorporating Low-Rank Adaptation (LoRA)\cite{hu2021loralowrankadaptationlarge} to fine-tune model weights with minimal computational overhead.
    \item DSI+QLoRA: Extending LoRA with quantization techniques \cite{dettmers2023qloraefficientfinetuningquantized} for enhanced efficiency.
    \item DSI+ALoRA: Adapting LoRA \cite{liu2024aloraallocatinglowrankadaptation} dynamically based on task-specific requirements for improved flexibility.
    \item DSI+ConvoLoRA: Introducing convolutional layers within the LoRA \cite{aleem2024convloraadabnbaseddomain} framework to capture local patterns and context.
\end{itemize}

The DSI model operates within a \textbf{multi-task learning} framework, where \textbf{indexing} and \textbf{retrieval} are treated as distinct yet complementary tasks. This dual-task approach enhances model robustness, mutually reinforcing the system's overall effectiveness.
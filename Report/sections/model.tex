\subsection{Model Architecture Novelties}

We adopt T5 as our core architecture, but additionally optimize training and memory usage by applying three key techniques: \textbf{Dynamic Pruning}, \textbf{LoRA}, and \textbf{QLoRA}.

\subsubsection{Dynamic Pruning}
Large-scale DSI frameworks, especially when operating on extensive datasets like MS-MARCO, can consume considerable memory and runtime. While fixed or static pruning methods reduce computational overhead, they also risk discarding parameters essential to retrieval accuracy.

In response, we propose \textbf{Dynamic Pruning}, a type of \textbf{unstructured pruning} which adaptively prunes weights based on real-time feature-importance scores. Concretely:
\begin{itemize}
    \item \textbf{Feature Scoring:} At each training step, the model computes importance indicators (e.g., from attention distributions) that highlight which features most strongly impact retrieval.
    \item \textbf{Dynamic Cutoffs:} A context-dependent threshold then prunes low-importance features while preserving the most influential parameters. Specifically, weights with the smallest magnitudes (by L1 norm) are set to zero.
    \item \textbf{Iterative Refinement:} Throughout training, the thresholds and importance metrics are continuously updated, balancing the removal of redundant weights with the retention of crucial ones.
\end{itemize}
By removing less critical parameters, unstructured pruning substantially lowers memory and computational demands without degrading retrieval performance. In our experiments, Dynamic Pruning—used alongside Mixed Precision and Gradient Accumulation—shortened training time from over five hours to just 45 minutes for each epoch (considering the whole dataset). 

\begin{table*}[h]
    \centering
    \begin{tabular}{p{2.1cm}p{2.8cm}p{3.0cm}p{5.5cm}}
    \toprule
    \textbf{Technique} & \textbf{Key Idea} & \textbf{Memory/Params Impact} & \textbf{Advantages / Notes} \\
    \midrule
    
    \textbf{LoRA} 
    & Inserts low-rank adapters in the attention layers
    & Very few additional trainable parameters 
    & Parameter-efficient fine-tuning; simpler training, fast convergence. \\
    
    \textbf{QLoRA} 
    & Quantizes base model to 4-bit and LoRA
    & Smaller memory footprint than LoRA 
    & Maintains higher-precision adapters for fine-tuning while cutting base-model GPU usage. \\
    
    \textbf{AdaLoRA} 
    & Dynamically adjusts the rank of the LoRA adapters 
    & Freezes base  and variable-rank adapters 
    & Allows higher rank initially for better capacity, then prunes rank to mitigate overfitting.  \\
    
    \textbf{ConvLoRA (LoCon)} 
    & Depthwise convolution layer before low-rank projection 
    & Small convolution and low-rank adapter 
    & Better local token-level pattern modeling than LoRA \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{LoRA Family Comparison} these techniques are used for efficient T5-based DSI fine-tuning.}
    \label{tab:comparison}
    \end{table*}
    
\subsubsection{Parameter-Efficient Fine-Tuning Techniques}
Beyond pruning, we integrate parameter-efficient fine-tuning approaches. The techniques we explored include:

\begin{itemize}
    \item \textbf{LoRA (Low-Rank Adaptation):} LoRA freezes the original T5 weights and injects small low-rank adapter modules into the attention layers. During training, only these adapter weights are updated—dramatically cutting the total trainable parameter count and thus reducing memory needs and speeding up convergence.

    \item \textbf{QLoRA (4-bit Quantization for T5):} To further minimize memory consumption, we apply \textbf{QLoRA}, which quantizes the T5 weights to a \emph{4-bit} format using the \textit{bitsandbytes} library. In this setup, the base T5 is compressed into 4-bit precision, while LoRA’s low-rank adapters remain at higher precision for effective fine-tuning. This setup reduces memory requirements and training overhead without sacrificing retrieval accuracy.

    \item \textbf{AdaLoRA (Adaptive LoRA):} Unlike standard LoRA, which uses a fixed rank, AdaLoRA dynamically adjusts the rank of the LoRA adapters during training. Initially, the model allocates a higher rank (\texttt{init\_r}) to capture sufficient task-specific information. After a warm-up phase, the method prunes less important dimensions, gradually reducing the rank (\texttt{target\_r}). This process minimizes trainable parameters while mitigating overfitting, making it ideal for large-scale models with constrained GPU memory.

    \item \textbf{ConvLoRA (LoCon):} A variant of LoRA that incorporates a depthwise convolutional layer before applying the low-rank projection. Originally designed for image-generation models, this extension enhances local feature modeling in text-based tasks by capturing token-level dependencies. The convolutional kernel size (\texttt{conv\_kernel\_size}) and rank (\texttt{conv\_lora\_rank}) are key hyperparameters that control the adaptation behavior.
\end{itemize}

\subsubsection{Comparison of Techniques}
Table~\ref{tab:comparison} summarizes the main differences among the proposed methods for memory-efficient fine-tuning and optimization. We highlight key ideas, how each method impacts model parameters and memory usage, and any special advantages or considerations.



In our work, we have used the first version of the MS Marco Dataset \cite{bajaj2018msmarcohumangenerated}, which is composed by 100k real Bing questions and human generated answers. The dataset is already partitioned into Training (82326 samples), Validation (10047 samples) and Test (9650 samples). Each partition is organized as a dictionary where the most important keys are the following:
\begin{itemize}
    \item \textbf{answers}: the answer related to the query based on the text informations.
    \item \textbf{passages}: contains another dictionary where we can find the complete corpus of the text and each passage that composes it.
    \item \textbf{query}: it is the question asked.
\end{itemize}
Since we are interested in the ranking of the documents, we have used the SimpleSearcher from pyserini \cite{pyserini}, in order to obtain the most relevant 1000 documents. 
The pre-processing applied to the dataset is explained in the following subsections.

Starting from the dataset, we have computed the maximum length of the inputs of the encoder (1797) and decoder (4), which will be useful during the tokenization process. We have used the pretrained tokenizer from the small version of the T5 model \cite{raffel2023exploringlimitstransferlearning}. Our tokenized dataset is composed only by the following parts:
\begin{itemize}
    \item \textbf{Query}: tokenized version of the original query
    \item \textbf{Query and Corpus}: tokenized input text, which is composed by the concatenation of the query and the corpus
    \item \textbf{Document IDs}: tokenized version of the identificator of each document
    \item \textbf{Ranked Document IDs}: tokenized version of the ranked list of the first 1000 document ids
\end{itemize}
\subsection{DSI-QG-Merge Dataset Generation}
In order to obtain a better dataset for our model we followed the generation of the DSI-QG-Merge dataset as described in Chen et al. \cite{chen2023understandingdifferentialsearchindex}. The DSI-QG-Merge dataset was generated using a two-phase procedure to improve the completeness of document representations. First, each document in the corpus was divided into segments, and pseudo-queries were generated for these segments using a query generation model (docT5query). This produced a dictionary of text fragments and their corresponding pseudo-queries. Furthermore, to decrease the magnitude of the resulting dataset and ensure high-quality training data, a dense retrieval model was then used to filter these fragments, retaining only those segments that accurately recalled their original document identifiers. The resulting dataset is comprised of: filtered fragments, pseudo-queries, and original training queries, providing a relevant training corpus for the DSI-QG-Merge model.

In particular, for the docids generation, we optef for a semantically structured mechanism: the docid should be able to meaningful some informations related to the semantics of the associated document. To do this, we have followed the algorithm provided by \cite{tay2022transformermemorydifferentiablesearch} and reported in Appendix \ref{sec:semanticids}. 

\subsection{Data Augmentation}

\input{sections/augmentation.tex}

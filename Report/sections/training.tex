The fine-tuning experiments were conducted using PyTorch Lightning, in which we employed automatic checkpointing (every 4 batches), and mixed precision computation. For all experiments, we set the LoRA rank to 8 and used a scaling factor (\(\alpha\)) of 32. A dropout rate of 0.1 was applied to prevent overfitting. The model was trained using mixed 16-bit precision (FP16) to improve memory efficiency. We used the AdamW optimizer with a learning rate of \(5 \times 10^{-5}\), and the training was limited to a maximum of 5 epochs for obvious limitation in power. To prevent overfitting, early stopping was enabled (although not used here), terminating training after 5 consecutive epochs without validation loss improvement. 
To evaluate the efficiency of different fine-tuning techniques, we measured the number of trainable parameters, the total parameter count (including frozen parameters), the training time per 1,000 samples, and the validation time per 1,000 samples.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{l|rrrrrr}
    \toprule
    \textbf{Technique} & \textbf{\#Trainable Params} & \textbf{\#Total Params} & \textbf{Time (1K)} & \textbf{GPU Usage} & \textbf{ModelsSize} & \textbf{ValLoss} \\
    \midrule
    Base T5         & 60.5M & 60.5M & 8m & 10.7GB & 242.03MB & 20.75\\
    LoRA            & 294K & 60.8M & 3m & 8.6GB & 243.20MB & 3.70\\
    QLoRA           & 294K & 45.1M & 5m & 8.4GB & 180.29MB & 3.89\\
    ALoRA           & 1.6M & 62.1M & 10m & 9.1GB & 248.52MB & 8.93\\
    ConvLoRA (LoCon)& 6.1K & 60.5M & 2m 30s & 2.2GB & 242.05MB & 19.45\\
    \bottomrule
    \end{tabular}
    \caption{\textbf{MS-MARCO100K Fine-Tuning Techniques Comparison}: A comparison of training/validation times over 1000 samples, total parameter counts, and number of trainable parameters across different fine-tuning techniques.}
    \label{tab:results_comparison}
\end{table}



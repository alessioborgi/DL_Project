\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{subfiles}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\usepackage{algpseudocodex}
\usepackage{algorithm}
\usepackage{amsmath, amssymb}
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Running Title for Header}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Differentiable Search Indexing
%%%% Cite as
%%%% Update your official citation here when published 
\thanks{\textit{\underline{Citation}}: 
\textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Alessio Borgi, Eugenio Bugli, Damiano Imola \\
  1952442, 1934824, 2109063 \\
  Sapienza Universit√† di Roma \\
  Rome\\
  \texttt{\{borgi.1952442, bugli.1934824, imola.2109063\}@studenti.uniroma1.it} \\
}

\begin{document}
\maketitle

\begin{abstract}
\input{sections/abstract.tex}
\end{abstract}


% keywords can be removed
\keywords{DSI \and POS-MLM \and QLoRA \and Dynamic Pruning \and LoRA}

\section{Introduction} \subfile{sections/intro.tex}
\section{Dataset} \subfile{sections/data.tex}

\section{Model} \subfile{sections/model.tex}
\section{Training and Results} \subfile{sections/training.tex}

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}


\section{Conclusion}
In this work, we explored various Parameter-Efficient Fine-Tuning (PEFT) techniques for adapting a T5-based Dense Sparse Indexing (DSI) model to the MS-MARCO100K dataset. Our experiments evaluated LoRA, QLoRA, AdaLoRA, and ConvLoRA, assessing their efficiency in terms of trainable parameters, computational cost, and retrieval performance.

Future research will focus on extending these approaches to larger-scale retrieval datasets, exploiting additional peft methodologies further optimizing model efficiency for real-world applications.



%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}

\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{subfiles}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\usepackage{algpseudocodex}
\usepackage{algorithm}
\usepackage{amsmath, amssymb}
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Running Title for Header}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Differentiable Search Indexing
%%%% Cite as
%%%% Update your official citation here when published 
\thanks{\textit{\underline{Citation}}: 
\textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Alessio Borgi, Eugenio Bugli, Damiano Imola \\
  1952442, 1934824, 2109063 \\
  Sapienza Universit√† di Roma \\
  Rome\\
  \texttt{\{borgi.1952442, bugli.1934824, imola.2109063\}@studenti.uniroma1.it} \\
}

\begin{document}
\maketitle

\begin{abstract}
\input{sections/abstract.tex}
\end{abstract}


% keywords can be removed
\keywords{DSI \and POS-MLM \and QLoRA \and Dynamic Pruning \and LoRA}

\section{Introduction} \subfile{sections/intro.tex}
\section{Dataset} \subfile{sections/data.tex}

\section{Model} \subfile{sections/model.tex}
\section{Training and Results} \subfile{sections/training.tex}



\subsection{Figures} 
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}


\section{Conclusion}
In this work, we explored various \textbf{Parameter-Efficient Fine-Tuning (PEFT)} techniques for adapting a T5-based \textit{Dense Sparse Indexing} (DSI) model to the MS-MARCO100K dataset. Our experiments evaluated \textbf{LoRA}, \textbf{QLoRA}, \textbf{AdaLoRA}, and \textbf{ConvLoRA}, assessing their efficiency in terms of trainable parameters, computational cost, and retrieval performance.

Our findings indicate that \textbf{LoRA} serves as a strong baseline, significantly reducing memory requirements while maintaining performance. \textbf{QLoRA} further improves efficiency by applying \textbf{4-bit quantization}, making it particularly beneficial for memory-constrained environments. \textbf{AdaLoRA} dynamically adjusts rank allocation, striking a balance between performance and efficiency by progressively pruning less important low-rank dimensions. Finally, \textbf{ConvLoRA} introduces depthwise convolutions, enhancing local feature interactions at the cost of slightly increased computational overhead.

Overall, our results demonstrate that LoRA-based fine-tuning strategies provide an effective and scalable approach for adapting large language models to retrieval tasks. Adaptive methods such as \textbf{AdaLoRA} and \textbf{ConvLoRA} introduce further flexibility, optimizing rank allocation and feature extraction. Future research will focus on extending these approaches to larger-scale retrieval datasets and further optimizing model efficiency for real-world applications.



%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}

\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Running Title for Header}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Differentiable Search Indexing
%%%% Cite as
%%%% Update your official citation here when published 
\thanks{\textit{\underline{Citation}}: 
\textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Alessio Borgi, Eugenio Bugli, Damiano Imola \\
  1952442, 1934824, 2109063 \\
  Sapienza Universit√† di Roma \\
  Rome\\
  \texttt{\{borgi.1952442, bugli.1934824, imola.2109063\}@studenti.uniroma1.it} \\
}


\begin{document}
\maketitle


\begin{abstract}
\end{abstract}


% keywords can be removed
\keywords{First keyword \and Second keyword \and More}


\section{Introduction}
Trainable Information Retrieval (IR) systems are characterized by two phases:
\begin{itemize}
  \item \textbf{Indexing: }Indexing of a corpus, which means to associate the content of each document with its corresponding docid.
  \item \textbf{Retrieval: }Learn how to retrieve efficiently from the index, which means to 
\end{itemize}
Instead of using Contrastive Learning based Dual Ecnoders, the paper proposes an architecture which directly map a query \textbf{q} to a relevant docid \textbf{j}. This architecture, called DSI, it's implemented with a pre-trained Transformer and all the information of the corpus are encoded within the parameters of the language model. When we are doing inference, the give to the trained model a text query as input and we expect to obtain a docid as output. If we are interested in a ranked list of relevant documents we can also use Beam Search. Our DSI system uses standard model inference to map from encodings to docids, instead of learning internal representations that optimize a search procedure. DSI can be extended in different ways:
\begin{itemize}
  \item \textbf{Document representation: }there are several ways to represent documents (e.g. full text, bag-of-words representations, ...)
  \item \textbf{Docid representation: }(e.g. unique tokens, structured semantic docids, text strings, ...)
\end{itemize}

\section{Indexing Methods}
Given a sequence of document tokens, the model is trained to predict the docids. There can be used different strategies:
\begin{itemize}
  \item \textbf{Inputs2Target}: 
  \item \textbf{Targets2Inputs}:
  \item \textbf{Bidirectional}: 
\end{itemize}

\section{Filtering by Damiano}
You have dense model which are good to mantain the knowledge of the inputs.
Filtering: starting from a fragment, you use the dense model to perform ranking in order to obtain a list of k docid related to your document. You have obtained relevant fragments which will be inserted inside your training data.

\section{Document Representation Strategies}

\section{Docid Representation for Retrieval}

\section{Headings: first level}
\label{sec:headings}
See Section \ref{sec:headings}.

\subsection{Headings: second level}

\subsubsection{Headings: third level}

\paragraph{Paragraph}

\section{Examples of citations, figures, tables, references}
\label{sec:others}
\cite{pyserini}.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}


\subsection{Figures} 
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}

\subsection{Tables}
See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\section{Conclusion}
Your conclusion here

\section*{Acknowledgments}
This was was supported in part by......

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}

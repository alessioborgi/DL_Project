{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the MS MARCO dataset\n",
    "ms_marco = load_dataset(\"microsoft/msmarco\", \"v2.1\", split=\"train\")\n",
    "\n",
    "# Display a sample\n",
    "print(ms_marco[0])\n",
    "\n",
    "train_data = ms_marco.shuffle(seed=42).select(range(80000))\n",
    "validation_data = ms_marco.shuffle(seed=42).select(range(8000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load a pre-trained model for embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Generate embeddings for documents\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = [embed_text(doc['passage']) for doc in train_data]\n",
    "\n",
    "# Apply clustering\n",
    "clustering = AgglomerativeClustering(n_clusters=100)\n",
    "labels = clustering.fit_predict(embeddings)\n",
    "\n",
    "# Assign semantically structured identifiers\n",
    "ssids = [f\"Cluster_{label}\" for label in labels]\n",
    "train_data = [{'text': doc['passage'], 'docid': ssid} for doc, ssid in zip(train_data, ssids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DSIInputs2TargetDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        doc_text = self.data[idx]['text']\n",
    "        docid = self.data[idx]['docid']\n",
    "\n",
    "        # Tokenize input (document text)\n",
    "        source = self.tokenizer(\n",
    "            doc_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Tokenize target (structured identifier)\n",
    "        target = self.tokenizer(\n",
    "            docid,\n",
    "            max_length=10,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': source['input_ids'].squeeze(),\n",
    "            'attention_mask': source['attention_mask'].squeeze(),\n",
    "            'labels': target['input_ids'].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = DSIInputs2TargetDataset(train_data, tokenizer)\n",
    "eval_dataset = DSIInputs2TargetDataset(validation_data, tokenizer)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dsi_output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    model.eval()\n",
    "    hits_at_1 = 0\n",
    "    total = len(dataset)\n",
    "    \n",
    "    for data in dataset:\n",
    "        inputs = tokenizer(data['text'], return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs)\n",
    "        predicted_docid = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if predicted_docid == data['docid']:\n",
    "            hits_at_1 += 1\n",
    "\n",
    "    accuracy = hits_at_1 / total\n",
    "    print(f\"Hits@1 Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluate(model, eval_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
